{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "chunk = 1024 \n",
    "sample_format = pyaudio.paInt16  \n",
    "channels = 2\n",
    "fs = 44100  \n",
    "seconds = 5\n",
    "filename = \"output.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio() \n",
    "\n",
    "print('Recording')\n",
    "\n",
    "stream = p.open(format=sample_format,\n",
    "                channels=channels,\n",
    "                rate=fs,\n",
    "                frames_per_buffer=chunk,\n",
    "                input=True)\n",
    "\n",
    "frames = []  \n",
    "for i in range(0, int(fs / chunk * seconds)):\n",
    "    data = stream.read(chunk)\n",
    "    frames.append(data)\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "print('Finished recording')\n",
    "\n",
    "wf = wave.open(filename, 'wb')\n",
    "wf.setnchannels(channels)\n",
    "wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "wf.setframerate(fs)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m wf\u001b[38;5;241m.\u001b[39mreadframes(chunk)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m data \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     data \u001b[38;5;241m=\u001b[39m wf\u001b[38;5;241m.\u001b[39mreadframes(chunk)\n\u001b[0;32m     20\u001b[0m stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\amend\\anaconda3\\envs\\gesture\\Lib\\site-packages\\pyaudio\\__init__.py:550\u001b[0m, in \u001b[0;36mPyAudio.Stream.write\u001b[1;34m(self, frames, num_frames, exception_on_underflow)\u001b[0m\n\u001b[0;32m    547\u001b[0m     width \u001b[38;5;241m=\u001b[39m get_sample_size(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format)\n\u001b[0;32m    548\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channels \u001b[38;5;241m*\u001b[39m width))\n\u001b[1;32m--> 550\u001b[0m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[43mexception_on_underflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chunk = 1024  \n",
    "\n",
    "wf = wave.open(filename, 'rb')\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "\n",
    "stream = p.open(format = p.get_format_from_width(wf.getsampwidth()),\n",
    "                channels = wf.getnchannels(),\n",
    "                rate = wf.getframerate(),\n",
    "                output = True)\n",
    "\n",
    "\n",
    "data = wf.readframes(chunk)\n",
    "\n",
    "while data != '':\n",
    "    stream.write(data)\n",
    "    data = wf.readframes(chunk)\n",
    "\n",
    "stream.close()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "bundle = torchaudio.pipelines.HUBERT_ASR_LARGE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bundle.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = bundle.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "data, sample_rate = sf.read(filename, dtype='float32')\n",
    "waveform = torch.from_numpy(data)\n",
    "print(torchaudio.list_audio_backends())\n",
    "if len(waveform.shape) > 1:\n",
    "    waveform = waveform.mean(dim=1, keepdim=True)\n",
    "waveform=waveform.squeeze(1)\n",
    "waveform = waveform.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform=waveform.to('cpu')\n",
    "emissions, _ = model(waveform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchaudio.models.decoder import CTCDecoderLM, CTCDecoderLMState\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "log=emissions.detach().cpu()\n",
    "logits = log.permute(0, 2, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoded_indices, _ = torchaudio.functional.ctc_decode(logits, blank=0, greedy=True)\n",
    "from torchaudio.models.decoder import ctc_decoder\n",
    "labels = ('-', \n",
    " '|',\n",
    " 'E',\n",
    " 'T',\n",
    " 'A',\n",
    " 'O',\n",
    " 'N',\n",
    " 'I',\n",
    " 'H',\n",
    " 'S',\n",
    " 'R',\n",
    " 'D',\n",
    " 'L',\n",
    " 'U',\n",
    " 'M',\n",
    " 'W',\n",
    " 'C',\n",
    " 'F',\n",
    " 'G',\n",
    " 'Y',\n",
    " 'P',\n",
    " 'B',\n",
    " 'V',\n",
    " 'K',\n",
    " \"'\",\n",
    " 'X',\n",
    " 'J',\n",
    " 'Q',\n",
    " 'Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: [CTCHypothesis(tokens=tensor([18, 20, 25, 23, 11, 12, 25, 15, 24, 16, 15,  4,  7,  4,  5, 25, 26, 16,\n",
      "        18,  5, 20, 23, 15,  6, 23, 22,  1, 12, 13, 11, 12, 25, 28, 13, 23, 27,\n",
      "         1, 12, 25, 26, 12,  1, 22,  1, 23, 18, 24,  9, 10, 18, 15, 26, 10, 25,\n",
      "         9, 27,  1, 22, 26, 13, 24, 20, 15, 25, 13, 24, 12, 19, 16, 28, 23, 20,\n",
      "        21, 12, 14, 25,  5,  6, 17,  7,  9,  1, 21, 16, 10,  5, 17,  7, 15, 12]), words=[], score=194.26401501893997, timesteps=tensor([ 1,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20,\n",
      "        21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
      "        39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58,\n",
      "        59, 60, 61, 62, 63, 64, 65, 67, 69, 70, 71, 73, 74, 75, 76, 78, 79, 80,\n",
      "        81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
      "       dtype=torch.int32))]\n",
      "Final Transcription: GPXKDLXW'CWAIAOXJCGOPKWNKV|LUDLXZUKQ|LXJL|V|KG'SRGWJRXSQ|VJU'PWXU'LYCZKPBLMXONFIS|BCROFIWL\n"
     ]
    }
   ],
   "source": [
    "decoder = ctc_decoder(\n",
    "    lexicon=None,        \n",
    "    tokens=labels,\n",
    "    beam_size=10,        \n",
    "    lm=None,             \n",
    "    sil_token='-',  \n",
    "    blank_token='-'      \n",
    ")\n",
    "emissions = torch.randn(1, 100, len(labels))  \n",
    "decoder.decode_begin()\n",
    "decoder.decode_step(emissions[0, :50])  \n",
    "decoder.decode_step(emissions[0, 50:])  \n",
    "decoder.decode_end()\n",
    "result = decoder.get_final_hypothesis()\n",
    "print(f\"Transcription: {result}\")\n",
    "hypothesis = result[0]\n",
    "transcription = ''.join([labels[i] for i in hypothesis.tokens])\n",
    "print(\"Final Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: -D-A-D--D-A-D--\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H\n",
    "model = bundle.get_model()\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "filename = 'output.wav'  \n",
    "data, sample_rate = sf.read(filename, dtype='float32')\n",
    "waveform = torch.from_numpy(data)\n",
    "\n",
    "if waveform.ndim > 1:\n",
    "    waveform = waveform.mean(dim=1)\n",
    "\n",
    "if sample_rate != bundle.sample_rate:\n",
    "    waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "\n",
    "waveform = waveform.unsqueeze(0)  # Shape: [1, time]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    emissions, _ = model(waveform)\n",
    "\n",
    "emission = torch.log_softmax(emissions[0], dim=-1)\n",
    "tokens = torch.argmax(emission, dim=-1)\n",
    "\n",
    "transcription = []\n",
    "prev_token = None\n",
    "blank_token = labels.index('|') \n",
    "\n",
    "for token in tokens:\n",
    "    if token != prev_token and token != blank_token:\n",
    "        transcription.append(labels[token])\n",
    "    prev_token = token\n",
    "\n",
    "transcribed_text = ''.join(transcription)\n",
    "print(\"Transcription:\", transcribed_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
